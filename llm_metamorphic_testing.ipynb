{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LLM Query Outputs with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# We are using one of the very popular small yet efficient Transformer model for computing embeddings for our texts. \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the embedding vector for the given text.\n",
    "    \"\"\"\n",
    "    embedding = model.encode([text])\n",
    "    return embedding\n",
    "\n",
    "def compute_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors. Returns a value between -1 and 1.\n",
    "    \"\"\"\n",
    "    return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "def simulate_llm_output(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulates an LLM query response. In a real-world scenario, this function would call an LLM API.\n",
    "    \"\"\"\n",
    "    # For demonstration, we return a predefined response based on the query content\n",
    "    if \"drawbacks\" in query:\n",
    "        return \"Dining outside exposes you to unpredictable weather and increases the risk of foodborne illnesses\"\n",
    "    elif \"negative\" in query:\n",
    "        return \"Eating outdoors subjects you to variable weather conditions and raises the potential for foodborne diseases\"\n",
    "    elif \"positive\" in query:\n",
    "        return \"Dining outdoors can improve your mood and offer a refreshing change from routine indoor meals.\"\n",
    "    else:\n",
    "        return \"I am sorry, I do not have information on that.\"\n",
    "\n",
    "def interpret_similarity(cos_sim: float) -> str:\n",
    "    \"\"\"\n",
    "    Interprets the cosine similarity value (ranging from -1 to 1) and returns a description\n",
    "    of the relationship between two texts.\n",
    "\n",
    "    Parameters:\n",
    "        cos_sim (float): Cosine similarity value between -1 and 1.\n",
    "\n",
    "    Returns:\n",
    "        str: A string description indicating if the texts are highly similar, not related, or opposite.\n",
    "    \"\"\"\n",
    "    # Define threshold values for interpretation\n",
    "    if cos_sim >= 0.8:\n",
    "        return \"The texts are highly similar.\"\n",
    "    elif cos_sim <= -0.8:\n",
    "        return \"The texts are opposite.\"\n",
    "    else:\n",
    "        return \"The texts are not closely related.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What are the drawbacks of eating outside?\n",
      "Query 2: What negative aspects come with outdoor dining?\n",
      "Output for query_similar_1: Dining outside exposes you to unpredictable weather and increases the risk of foodborne illnesses\n",
      "Output for query_similar_2: Eating outdoors subjects you to variable weather conditions and raises the potential for foodborne diseases\n",
      "Cosine similarity between the outputs: 0.8508991003036499\n",
      "The texts are highly similar.\n"
     ]
    }
   ],
   "source": [
    "# Test1 : if we give 2 similar queries, LLM output should be similar.\n",
    "query_similar_1 = \"What are the drawbacks of eating outside?\"\n",
    "query_similar_2 = \"What negative aspects come with outdoor dining?\"\n",
    "print(f\"Query 1: {query_similar_1}\")\n",
    "print(f\"Query 2: {query_similar_2}\")\n",
    "output_similar_1 = simulate_llm_output(query_similar_1)\n",
    "output_similar_2 = simulate_llm_output(query_similar_2)\n",
    "print(f\"Output for query_similar_1: {output_similar_1}\")\n",
    "print(f\"Output for query_similar_2: {output_similar_2}\")\n",
    "embedding_similar_1 = get_embedding(output_similar_1)\n",
    "embedding_similar_2 = get_embedding(output_similar_2)\n",
    "similarity_similar = compute_cosine_similarity(embedding_similar_1, embedding_similar_2)\n",
    "print(f\"Cosine similarity between the outputs: {similarity_similar}\")\n",
    "print(interpret_similarity(similarity_similar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What are the drawbacks of eating outside?\n",
      "Query 2: What positive aspects come with outdoor dining?\n",
      "Output for query1: Dining outside exposes you to unpredictable weather and increases the risk of foodborne illnesses\n",
      "Output for query2: Dining outdoors can improve your mood and offer a refreshing change from routine indoor meals.\n",
      "Cosine similarity between the outputs: 0.6589205265045166\n",
      "The texts are not closely related.\n"
     ]
    }
   ],
   "source": [
    "# Test1 : if we give 2 opposite / unrelated queries, LLM output should not be similar.\n",
    "query1 = \"What are the drawbacks of eating outside?\"\n",
    "query2 = \"What positive aspects come with outdoor dining?\"\n",
    "print(f\"Query 1: {query1}\")\n",
    "print(f\"Query 2: {query2}\")\n",
    "output1 = simulate_llm_output(query1)\n",
    "output2 = simulate_llm_output(query2)\n",
    "print(f\"Output for query1: {output1}\")\n",
    "print(f\"Output for query2: {output2}\")\n",
    "embedding1 = get_embedding(output1)\n",
    "embedding2 = get_embedding(output2)\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity between the outputs: {similarity}\")\n",
    "print(interpret_similarity(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for query1: She is really excited about her promotion\n",
      "Output for query2: She is not really excited about her promotion\n",
      "Cosine similarity between the outputs: 0.9077736139297485\n",
      "The texts are highly similar.\n"
     ]
    }
   ],
   "source": [
    "# Test3 : If we give something like below, LLM output should not be opposite.\n",
    "output1 = \"She is really excited about her promotion\"\n",
    "output2 = \"She is not really excited about her promotion\"\n",
    "print(f\"Output for query1: {output1}\")\n",
    "print(f\"Output for query2: {output2}\")\n",
    "embedding1 = get_embedding(output1)\n",
    "embedding2 = get_embedding(output2)\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity between the outputs: {similarity}\")\n",
    "print(interpret_similarity(similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM lacks to understand deeper meaning of the 2 sentences and hence it is giving high similarity, so some limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for query1: Oh great, another rainy day!!\n",
      "Output for query2: Oh great, another day at the beach, I am so lucky\n",
      "Cosine similarity between the outputs: 0.5919630527496338\n",
      "The texts are not closely related.\n"
     ]
    }
   ],
   "source": [
    "# Test4 : If we give something like below to test nuances or emotions, LLM output should not be opposite.\n",
    "output1 = \"Oh great, another rainy day!!\"\n",
    "output2 = \"Oh great, another day at the beach, I am so lucky\"\n",
    "print(f\"Output for query1: {output1}\")\n",
    "print(f\"Output for query2: {output2}\")\n",
    "embedding1 = get_embedding(output1)\n",
    "embedding2 = get_embedding(output2)\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity between the outputs: {similarity}\")\n",
    "print(interpret_similarity(similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM lacks to understand nuances, or emotions so another limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook illustrates a metamorphic testing approach for LLM query outputs using cosine similarity. By simulating LLM responses for different queries and comparing their semantic similarity, we can verify whether the models outputs adhere to the expected relationships-without relying on fixed or deterministic expected outputs.\n",
    "\n",
    "Such an approach is especially useful when working with non-deterministic LLM outputs where traditional testing methods may fall short.\n",
    "\n",
    " However, they have some limitations as mentioned above\n",
    " - Lack to understand deeper meaning of 2 texts.\n",
    " - Lack to understand emotions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
